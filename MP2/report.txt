EE5351 MP2
Lexi MacLean macle119

For this Machine Problem I implemented a single kernel function (MatrixMulKernel) and its wrapper function (MatrixMulOnDevice) to perform matrix multiplication.
The wrapper function loads the provided arrays into device memory, then calls the kernel function in an appropriate grid to multiply the entire product matrix.
The kernel function then computes the sums for the part of the product matrix in tiles.
Each block of threads first loads a tile of the input matricies into the sm-local shared memory, then computes partial sums for that tile, then moves onto the next.
The method employed here improves upon last weeks matrix multiplication by improving memory access patterns and allowing for arbitrarily sized matricies.
This method coalesces memory requests from adjacent threads in each warp, which should take advantage of the bursting characteristics of the memory bus, and stores a tile of memory from the factor matricies in the block's shared memory, which is much faster than the global memory where the matricies are.

On my personal computer with an RTX 3080ti, on compute compatibility 8.6, I get only negligible performance increase from the naive solution (about 30% speedup).
On the lab machine with a GTX1080 on compute compatibility 6.1, I get a much more substantial speedup, about 5-10x faster than the naive solution.
This increase is only achieved with a tile-width of 32, which maximizes coalesced memory requests, having the most adjacent threads requesting adjacent memory.
This is also though, the largest tile-width possible, since any bigger would not allow all the threads to fit in a single block.
Smaller tile widths are less performant, with a tile-width of 8 showing as-fast results as the naive solution (which has similar performance regardless of tile size).


I assume this discrepancy is due to newer automatic caching introduced in the interim.


Tiled Matrix Multiplication

    In your kernel implementation, how many threads can be simultaneously executing
    on a GeForce GTX 1080 GPU, which contains 20 Streaming Multiprocessors. Use
    nvcc --ptxas-options="-v" matrixmul_kernel.cu to see the resource usage of 
    your kernel (although compilation will fail, it will only do so after
    compiling the kernel and displaying the relevant information.)

The output of `nvcc -gencode=arch=compute_61,code=\"sm_61,compute_61\" --ptxas-options="-v" matrixmul_kernel` on the lab machine says that 28 registers, 8192 bytes smem, and 392 bytes cmem are used
my understanding is that the register number is is per-thread
The max number of registers per multiprocessor on compute capability 6.1 is 64K (i dont know if this is Ki or K... :( ), 20*64000/28 = 45714 threads
The number of threads allowed outright by the multiprocessor limitation is only 2048 * 20 = 40960, however, which is a bit lower, and I believe this will be the limit.

Nsight though gave me a slightly different answer...saying I would be limited in occupancy by the registers, but it seems to be working with a different registers/thread number.
